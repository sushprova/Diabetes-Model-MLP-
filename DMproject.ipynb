{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtptYCfKDmeFHoFYf7T8Mi"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4Wq8LsUfqPo",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "# import lime.explanation\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import sys\n",
        "import torch\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "from pathlib import PosixPath\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score\n",
        ")\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from time import time\n",
        "from torch import nn\n",
        "from torch.utils.data import (\n",
        "    DataLoader,\n",
        "    Dataset,\n",
        "    ConcatDataset\n",
        ")\n",
        "from tqdm import tqdm\n",
        "from typing import List, Literal, Optional, Tuple, Union\n",
        "\n",
        "# import lime\n",
        "# from lime import lime_tabular\n",
        "\n",
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s | [%(levelname)-8s] | %(message)s\",\n",
        "    datefmt='%Y-%m-%d %H:%M:%S',\n",
        "    handlers= [\n",
        "        logging.FileHandler(f'session-{datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\")}.log'),\n",
        "        logging.StreamHandler(sys.stdout)\n",
        "    ]\n",
        ")\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "LOGGER = logging.getLogger(\"diabetes_trainer\")\n",
        "LOGGER.setLevel(logging.DEBUG)\n",
        "\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "LOGGER.info(f\"Running with device: {DEVICE}\")\n",
        "\n",
        "FEATURE_COUNT = 5\n",
        "OUTPUT_OUTCOME_COUNT = 4\n",
        "\n",
        "'''-------------------------------------------------------------------------------------'''\n",
        "\n",
        "class SimpleDiabetesModel(nn.Module):\n",
        "    '''\n",
        "    Sets up a very simple and naive MLP model with one hidden ReLU layer of 20 nodes.\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(SimpleDiabetesModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(FEATURE_COUNT, 20)\n",
        "        self.fc2 = nn.Linear(20, OUTPUT_OUTCOME_COUNT)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # Performs forward pass.\n",
        "        # Shape of x as input: I think it should be [rows x FEATURE_COUNT]?\n",
        "        # Shape of x as output: should be [rows x OUTPUT_OUTCOME_COUNT]?\n",
        "        x = nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class DiabetesDataset(Dataset):\n",
        "    \"\"\"Diabetes dataset\"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_file: Optional[Union[str, os.PathLike, PosixPath]] = None,\n",
        "            premade_dataset: Optional[pd.DataFrame] = None\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Setups the diabetes dataset. Two options are available - either use a CSV file, or use a premade DataFrame.\n",
        "        For any instance of this, an input must be provided - ValueError will be raised if no input is provided.\n",
        "\n",
        "        To use the CSV file:\n",
        "\n",
        "        ```python\n",
        "        data = DiabetesDataset('file.csv')\n",
        "        ```\n",
        "\n",
        "        To use a previously made dataset on the memory\n",
        "\n",
        "        ```python\n",
        "        # Assume that the premade dataset is named merged_dataset in the code\n",
        "        data = DiabetesDataset(premade_dataset=merged_dataset)\n",
        "        ```\n",
        "\n",
        "        NOTE: The dataset is assumed to have six columns - 'age', 'gender', 'bmi', 'blood_glucose_level', 'hypertension' and 'diabetes'\n",
        "\n",
        "        Args:\n",
        "            input_file (Optional[Union[str, os.PathLike, PosixPath]], optional): An input file containing the dataset information. Defaults to None.\n",
        "            premade_dataset (Optional[pd.DataFrame], optional): A premade dataset on the memory. Defaults to None.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: Raised when there is no input file or premade dataset provided.\n",
        "        \"\"\"\n",
        "        if input_file is None and premade_dataset is None:\n",
        "            raise ValueError(\"An input source must be provided as a dataset.\")\n",
        "        if input_file is not None:\n",
        "            self.data: pd.DataFrame = pd.read_csv(input_file)\n",
        "        elif premade_dataset is not None:\n",
        "            self.data = premade_dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        features = torch.tensor(self.data.loc[index, ['age', 'gender', 'bmi', 'blood_glucose_level', 'hypertension']], dtype=torch.float32)\n",
        "        output = torch.tensor(self.data.loc[index, 'diabetes'], dtype=torch.long)\n",
        "        return features, output\n",
        "\n",
        "\n",
        "def setup_adam_optimizer(model: nn.Module, learning_rate: float = 0.001):\n",
        "    \"\"\"\n",
        "    Setups Adam optimizer for training.\n",
        "    Adam optimizer adjusts the learning rate for each param individually\n",
        "        based on the gradients and their moving averages.\n",
        "    This optimizer is also more efficient than gradient descent.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model for training.\n",
        "        learning_rate (float, optional): The starting learning rate for training. Defaults to 0.001.\n",
        "    \"\"\"\n",
        "    # we don't have to worry about other params, yet.\n",
        "    # keeps things simple first... please\n",
        "    return torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "def setup_cross_entropy_loss():\n",
        "    \"\"\"\n",
        "    Setups Cross Entropy Loss for training.\n",
        "    PyTorch's CrossEntropyLoss already has log-softmax in the calculation, which explains why this\n",
        "    implementation of the model doesn't have softmax.\n",
        "    \"\"\"\n",
        "    return nn.CrossEntropyLoss().to(DEVICE)\n",
        "\n",
        "def epoch_time(start_time: float, end_time: float) -> Tuple[int, int]:\n",
        "    \"\"\"\n",
        "    Calculates the time it takes for each epoch in minutes and seconds.\n",
        "\n",
        "    Args:\n",
        "        start_time (float): The point of start time.\n",
        "        end_time (float): The point of end time\n",
        "    \"\"\"\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "def split_dataset(\n",
        "    input_file: Union[str, os.PathLike, PosixPath, pd.DataFrame],\n",
        "    splits: List[float]\n",
        ") -> Tuple[DiabetesDataset]:\n",
        "    \"\"\"\n",
        "    Splits the dataset into train, test and validation sets,\n",
        "    while ensuring roughly equal original distribution of the dataset.\n",
        "    Args:\n",
        "        input_file (Union[str, os.PathLike, PosixPath, pd.DataFrame]): Input data for the training\n",
        "        splits (List[float]): The ratio of each subset for splitting.\n",
        "            In order: train, test and val splitting.\n",
        "            The list must have three elements.\n",
        "\n",
        "    Return: DiabetesDataset instances of the splitted dataset.\n",
        "    \"\"\"\n",
        "    assert len(splits) == 3, \"The splits must have all three subset splits.\"\n",
        "\n",
        "    if isinstance(input_file, pd.DataFrame):\n",
        "        dataset = input_file\n",
        "    else:\n",
        "        dataset = pd.read_csv(input_file)\n",
        "\n",
        "    # features = dataset[['age', 'gender', 'bmi', 'blood_glucose_level', 'hypertension']]\n",
        "    output = dataset['diabetes']\n",
        "\n",
        "    train_split, test_split, val_split = splits\n",
        "    # round 1\n",
        "    train_subset, test_subset = train_test_split(\n",
        "        dataset,\n",
        "        test_size=test_split,\n",
        "        random_state=SEED,\n",
        "        stratify=output\n",
        "    )\n",
        "    train_output = train_subset['diabetes']\n",
        "    # round 2\n",
        "    train_subset, val_subset = train_test_split(\n",
        "        train_subset,\n",
        "        test_size=val_split/train_split,\n",
        "        random_state=SEED,\n",
        "        stratify=train_output\n",
        "    )\n",
        "\n",
        "    train_subset.reset_index(drop=True, inplace=True)\n",
        "    test_subset.reset_index(drop=True, inplace=True)\n",
        "    val_subset.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    train_ds = DiabetesDataset(premade_dataset=train_subset)\n",
        "    test_ds = DiabetesDataset(premade_dataset=test_subset)\n",
        "    val_ds = DiabetesDataset(premade_dataset=val_subset)\n",
        "\n",
        "    LOGGER.info(f\"Post-split statistics:\")\n",
        "    LOGGER.info(f\"length of train: {len(train_ds)}\")\n",
        "    LOGGER.info(f\"ratio of train: {len(train_ds)/len(dataset)}\")\n",
        "    LOGGER.info(f\"distribution of train:\\n{train_subset['diabetes'].value_counts()/len(train_subset)}\")\n",
        "    LOGGER.info(f\"length of test: {len(test_ds)}\")\n",
        "    LOGGER.info(f\"ratio of test: {len(test_ds)/len(dataset)}\")\n",
        "    LOGGER.info(f\"distribution of test:\\n{test_subset['diabetes'].value_counts()/len(test_subset)}\")\n",
        "    LOGGER.info(f\"length of val: {len(val_ds)}\")\n",
        "    LOGGER.info(f\"ratio of val: {len(val_ds)/len(dataset)}\")\n",
        "    LOGGER.info(f\"distribution of val:\\n{val_subset['diabetes'].value_counts()/len(val_subset)}\")\n",
        "\n",
        "\n",
        "    return train_ds, test_ds, val_ds\n",
        "\n",
        "\n",
        "def train_and_eval(\n",
        "    input_file: Union[str, os.PathLike, PosixPath, pd.DataFrame],\n",
        "    model: nn.Module,\n",
        "    training_mode: Literal['naive', 'kfold'] = 'naive',\n",
        "    train_split: float = 0.8,\n",
        "    test_split: float = 0.1,\n",
        "    val_split: float = 0.1,\n",
        "    batch_size: int = 64,\n",
        "    learning_rate: float = 0.001,\n",
        "    epochs: int = 2,\n",
        "    n_folds: int = 10\n",
        "):\n",
        "    \"\"\"\n",
        "    Setting up training for the model.\n",
        "\n",
        "    Args:\n",
        "        input_file (Union[str, os.PathLike, PosixPath, pd.DataFrame]): Input data for the training\n",
        "        training_mode (Literal['naive', 'kfold'], optional): Training mode for the model.\n",
        "            - 'naive' for a classic epoch-based training.\n",
        "            - 'kfold' for training with k-Fold Cross Validation.\n",
        "            Defaults to 'naive'.\n",
        "        train_split (float, optional): The split for the train set, if training with 'naive'. Defaults to 0.8.\n",
        "        test_split (float, optional): The split for the test set, if training with 'naive'. Defaults to 0.1.\n",
        "        val_split (float, optional): The split for the validation set, if training with 'naive'. Defaults to 0.1.\n",
        "        batch_size (int, optional): The batch size for training. Defaults to 64.\n",
        "        learning_rate (float, optional): The learning rate for training. Defaults to 0.001.\n",
        "        epochs (int, optional): The number of epochs to run the training. Defaults to 2.\n",
        "        n_folds (int, optional): For training with k-fold Cross Validation, how many folds there will be. Defaults to 10.\n",
        "    \"\"\"\n",
        "    #dataset = DiabetesDataset(input_file)\n",
        "    # model = SimpleDiabetesModel()\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    # seeded_generator = torch.Generator().manual_seed(SEED)\n",
        "    train_subset, test_subset, val_subset = split_dataset(input_file, [train_split, test_split, val_split])\n",
        "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    optimizer = setup_adam_optimizer(model, learning_rate)\n",
        "    loss_func = setup_cross_entropy_loss()\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    trained_model_names = []\n",
        "\n",
        "    if training_mode == 'naive':\n",
        "        for epoch in tqdm(range(epochs), desc='Training'):\n",
        "            start_time = time()\n",
        "\n",
        "            # actually start training\n",
        "            model.train()\n",
        "            epoch_loss = 0\n",
        "            current_loss = 0\n",
        "            for i, batch in enumerate(train_loader):\n",
        "                feature, target = batch\n",
        "                feature = feature.to(DEVICE)\n",
        "                target = target.to(DEVICE)\n",
        "\n",
        "                output = model(feature)\n",
        "                loss = loss_func(output, target)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                epoch_loss += loss.item()\n",
        "                current_loss += loss.item()\n",
        "                if i % 500 == 499:\n",
        "                    print('Loss after mini-batch %5d: %.3f' %\n",
        "                        (i + 1, current_loss / 500))\n",
        "                    current_loss = 0.0\n",
        "\n",
        "            train_loss = epoch_loss / len(train_loader.dataset)\n",
        "\n",
        "            model.eval()\n",
        "            epoch_eval_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for i, batch in enumerate(val_loader):\n",
        "                    feature, target = batch\n",
        "                    feature = feature.to(DEVICE)\n",
        "                    target = target.to(DEVICE)\n",
        "\n",
        "                    output = model(feature)\n",
        "                    eval_loss = loss_func(output, target)\n",
        "\n",
        "                    epoch_eval_loss += eval_loss.item()\n",
        "\n",
        "            val_loss = epoch_eval_loss / len(val_loader.dataset)\n",
        "            end_time = time()\n",
        "            epoch_time_min, epoch_time_sec = epoch_time(start_time, end_time)\n",
        "\n",
        "            if val_loss < best_loss:\n",
        "                best_loss = val_loss\n",
        "                torch.save(model.state_dict(), 'diabetes_model_ckpt.pth')\n",
        "\n",
        "            LOGGER.info(f'Epoch: {epoch + 1} | Time: {epoch_time_min}m {epoch_time_sec}s')\n",
        "            LOGGER.info(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "            LOGGER.info(f'\\tVal Loss: {val_loss:.3f}')\n",
        "\n",
        "            # after training, we immediately evaluate the trained model\n",
        "\n",
        "        trained_model_names.append('diabetes_model_ckpt.pth')\n",
        "\n",
        "        model.load_state_dict(torch.load('diabetes_model_ckpt.pth', weights_only=True))\n",
        "        LOGGER.info(\"Loaded trained model for evaluation.\")\n",
        "        if DEVICE == 'cuda':\n",
        "            model = model.cuda()\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        true_target = []\n",
        "        predicted_target = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(test_loader):\n",
        "                feature, target = batch\n",
        "                feature = feature.to(DEVICE)\n",
        "                target = target.to(DEVICE)\n",
        "\n",
        "                output = model(feature)\n",
        "                true_target.append(target.detach().cpu().data.numpy())\n",
        "                predicted_target.append(output.detach().cpu().data.numpy())\n",
        "\n",
        "        true_target = np.concatenate(true_target)\n",
        "        predicted_target = np.concatenate(predicted_target, axis=0).argmax(axis=1)\n",
        "\n",
        "        LOGGER.info(f\"Classification Report for the model:\")\n",
        "        LOGGER.info(classification_report(true_target, predicted_target))\n",
        "\n",
        "    else:\n",
        "        kfold = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
        "        combined_train_subset = ConcatDataset([train_subset, val_subset, test_subset])\n",
        "        accuracies = []\n",
        "        f1s = []\n",
        "        precisions = []\n",
        "        recalls = []\n",
        "        for fold, (train_idx, val_idx) in enumerate(kfold.split(combined_train_subset)):\n",
        "            LOGGER.info(f\"Fold {fold + 1}/{n_folds}\")\n",
        "            train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
        "            val_sampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
        "\n",
        "            train_loader = DataLoader(combined_train_subset, batch_size=batch_size, sampler=train_sampler)\n",
        "            val_loader = DataLoader(combined_train_subset, batch_size=batch_size, sampler=val_sampler)\n",
        "\n",
        "            for epoch in tqdm(range(epochs), desc=f'Training with fold={fold+1}'):\n",
        "                start_time = time()\n",
        "                model.train()\n",
        "                epoch_loss = 0\n",
        "                current_loss = 0\n",
        "                for i, batch in enumerate(train_loader):\n",
        "                    feature, target = batch\n",
        "                    feature = feature.to(DEVICE)\n",
        "                    target = target.to(DEVICE)\n",
        "\n",
        "                    output = model(feature)\n",
        "                    loss = loss_func(output, target)\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    epoch_loss += loss.item()\n",
        "                    current_loss += loss.item()\n",
        "                    if i % 500 == 499:\n",
        "                        print('Loss after mini-batch %5d: %.3f' %\n",
        "                            (i + 1, current_loss / 500))\n",
        "                        current_loss = 0.0\n",
        "\n",
        "                train_loss = epoch_loss / len(train_loader.dataset)\n",
        "                end_time = time()\n",
        "                epoch_time_min, epoch_time_sec = epoch_time(start_time, end_time)\n",
        "\n",
        "                LOGGER.info(f'Epoch: {epoch + 1} | Time: {epoch_time_min}m {epoch_time_sec}s')\n",
        "                LOGGER.info(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "\n",
        "            LOGGER.info('Starting testing')\n",
        "\n",
        "            # Saving the model\n",
        "            save_path = f'diabetes_model_fold_{fold}.pth'\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "\n",
        "            trained_model_names.append('diabetes_model_fold_{fold}.pth')\n",
        "\n",
        "            # Evaluation for this fold\n",
        "            model.load_state_dict(torch.load(f'diabetes_model_fold_{fold}.pth', weights_only=True))\n",
        "            LOGGER.info(\"Loaded trained model for evaluation.\")\n",
        "            if DEVICE == 'cuda':\n",
        "                model = model.cuda()\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            true_target = []\n",
        "            predicted_target = []\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for i, batch in enumerate(test_loader):\n",
        "                    feature, target = batch\n",
        "                    feature = feature.to(DEVICE)\n",
        "                    target = target.to(DEVICE)\n",
        "\n",
        "                    output = model(feature)\n",
        "                    true_target.append(target.detach().cpu().data.numpy())\n",
        "                    predicted_target.append(output.detach().cpu().data.numpy())\n",
        "\n",
        "            true_target = np.concatenate(true_target)\n",
        "            predicted_target = np.concatenate(predicted_target, axis=0).argmax(axis=1)\n",
        "\n",
        "            accuracy = accuracy_score(true_target, predicted_target)\n",
        "            f1 = f1_score(true_target, predicted_target, average='marco')\n",
        "            precision = precision_score(true_target, predicted_target, average='marco')\n",
        "            recall = recall_score(true_target, predicted_target, average='marco')\n",
        "\n",
        "            accuracies.append(accuracy)\n",
        "            f1s.append(f1)\n",
        "            precisions.append(precision)\n",
        "            recalls.append(recall)\n",
        "\n",
        "            LOGGER.info(f\"Accuracy score for fold {fold+1}: {accuracy:.2f}\")\n",
        "            LOGGER.info(f\"F1 score for fold {fold+1}: {f1:.2f}\")\n",
        "            LOGGER.info(f\"Precision score for fold {fold+1}: {precision:.2f}\")\n",
        "            LOGGER.info(f\"Recall score for fold {fold+1}: {recall:.2f}\")\n",
        "\n",
        "        LOGGER.info(f\"Average accuracy across {n_folds} folds: {sum(accuracies)/len(accuracies):.2f}\")\n",
        "        LOGGER.info(f\"Average f1 score across {n_folds} folds: {sum(f1s)/len(f1s):.2f}\")\n",
        "        LOGGER.info(f\"Average precision score across {n_folds} folds: {sum(precisions)/len(precisions):.2f}\")\n",
        "        LOGGER.info(f\"Average recall score across {n_folds} folds: {sum(recalls)/len(recalls):.2f}\")\n",
        "\n",
        "    return trained_model_names\n",
        "\n",
        "\n",
        "# def explain_model(\n",
        "#         model: nn.Module,\n",
        "#         model_name: str,\n",
        "#         explainer: lime_tabular.LimeTabularExplainer,\n",
        "#         test_sample: np.ndarray\n",
        "#     ):\n",
        "#     \"\"\"\n",
        "#     Attempts to explain the model with LIME\n",
        "\n",
        "#     Args:\n",
        "#         model (nn.Module): A PyTorch-based model for explaination\n",
        "#         model_name (str): The name of the model to load into.\n",
        "#         test_bundle (DataLoader): The testing set that needs to be examined.\n",
        "#     \"\"\"\n",
        "#     model.load_state_dict(torch.load(f'{model_name}', weights_only=True))\n",
        "\n",
        "    # def quick_predict(test: np.ndarray):\n",
        "    #     #print(test.dtype)\n",
        "    #     model.eval()\n",
        "    #     with torch.no_grad():\n",
        "    #         test = torch.from_numpy(test)\n",
        "    #         test = test.float()\n",
        "    #         #print(test.dtype)\n",
        "    #         test = test.to(DEVICE)\n",
        "    #         output = model(test)\n",
        "    #         probas = nn.functional.softmax(output, dim=1)\n",
        "    #         print(probas.shape)\n",
        "    #     #print(output.detach().cpu().data.numpy())\n",
        "    #     #print(probas)\n",
        "    #     print(probas.numpy())\n",
        "    #     return probas.numpy()\n",
        "\n",
        "    # exp: lime.explanation.Explanation = explainer.explain_instance(test_sample, quick_predict, num_features=5)\n",
        "    # plot = exp.as_pyplot_figure()\n",
        "    # plot.savefig('test.png')\n",
        "if __name__ == '__main__':\n",
        "    model = SimpleDiabetesModel()\n",
        "    train_and_eval(\n",
        "        'MergedDataset.csv',\n",
        "        model,\n",
        "        'naive',\n",
        "        epochs=10,\n",
        "    )"
      ]
    }
  ]
}